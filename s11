import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor

## LOADING DATA ##

df = pd.read_csv('dataset.csv')
print('-' * 30 + " DATASET " + '-' * 30)
print(df.head())

## UNDERSTANDING DATA ##

print('-' * 30 + " DATASET INFO " + '-' * 30)
print(df.info())
print('-' * 80 + "\n", df.describe())

## DATA PREPROCESSING ##

print('-' * 30 + " DATA PREPROCESSING " + '-' * 30)
print('Total Duplicates = ', df.duplicated().sum())  # Duplicates
print('Null values per Column\n', df.isnull().sum())  # Nulls
print('After Dropping Irrelevant Columns')  # Column selection
df.drop(['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name'], axis=1, inplace=True)
print(df.info())
df2 = df.copy()
df = pd.get_dummies(df, columns=['track_genre'], drop_first=True)  # Handling categorical variable

## EXPLORATORY DATA ANALYSIS

# Correlation plot
plt.figure(figsize=(16, 8))
sns.heatmap(df2.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# Popularity distribution
plt.figure(figsize=(16, 8))
sns.histplot(df['popularity'], kde=True, bins=30)
plt.title('Distribution of Popularity')
plt.xlabel('Popularity')
plt.ylabel('Frequency')
plt.show()

## MODELLING ##

print('-' * 30 + " MODELLING " + '-' * 30)

# Features and target
X = df.drop('popularity', axis=1)
y = df['popularity']

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear regression
lr_model = LinearRegression()
lr_model.fit(X_train_scaled, y_train)
y_pred_lr = lr_model.predict(X_test_scaled)
mse_lr = mean_squared_error(y_test, y_pred_lr)
print(f'Linear Regression MSE: {mse_lr:.2f}')

# Actual vs. predicted values
sample_size = 150
plt.figure(figsize=(16, 8))
plt.plot(y_test.values[:sample_size], label='Actual Values', marker='o')
plt.plot(y_pred_lr[:sample_size], label='Predicted Values', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Popularity')
plt.title('Linear Regression: Actual vs. Predicted Values (Sample)')
plt.legend()
plt.show()

# Random Forest Regressor
rf_model = RandomForestRegressor(random_state=666)
rf_model.fit(X_train_scaled, y_train)
y_pred_rf = rf_model.predict(X_test_scaled)
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'Random Forest MSE: {mse_rf:.2f}')

# Actual vs. predicted values
sample_size = 150
plt.figure(figsize=(16, 8))
plt.plot(y_test.values[:sample_size], label='Actual Values', marker='o')
plt.plot(y_pred_rf[:sample_size], label='Predicted Values', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Popularity')
plt.title('Random Forest Regressor: Actual vs. Predicted Values (Sample)')
plt.legend()
plt.show()

# Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(random_state=666)
gb_model.fit(X_train_scaled, y_train)
y_pred_gb = gb_model.predict(X_test_scaled)
mse_gb = mean_squared_error(y_test, y_pred_gb)
print(f'Gradient Boosting MSE: {mse_gb:.2f}')

# Actual vs. predicted values
sample_size = 150
plt.figure(figsize=(16, 8))
plt.plot(y_test.values[:sample_size], label='Actual Values', marker='o')
plt.plot(y_pred_gb[:sample_size], label='Predicted Values', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Popularity')
plt.title('Gradient Boosting Regressor: Actual vs. Predicted Values (Sample)')
plt.legend()
plt.show()

# Support Vector Regressor
svr_model = SVR()
svr_model.fit(X_train_scaled, y_train)
y_pred_svr = svr_model.predict(X_test_scaled)
mse_svr = mean_squared_error(y_test, y_pred_svr)
print(f'Support Vector Regressor MSE: {mse_svr:.2f}')

# Actual vs. predicted values
sample_size = 150
plt.figure(figsize=(16, 8))
plt.plot(y_test.values[:sample_size], label='Actual Values', marker='o')
plt.plot(y_pred_svr[:sample_size], label='Predicted Values', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Popularity')
plt.title('Support Vector Regressor: Actual vs. Predicted Values (Sample)')
plt.legend()
plt.show()

# K-Nearest Neighbors Regressor
knn_model = KNeighborsRegressor()
knn_model.fit(X_train_scaled, y_train)
y_pred_knn = knn_model.predict(X_test_scaled)
mse_knn = mean_squared_error(y_test, y_pred_knn)
print(f'K-Nearest Neighbors Regressor MSE: {mse_knn:.2f}')

# Actual vs. predicted values
sample_size = 150
plt.figure(figsize=(16, 8))
plt.plot(y_test.values[:sample_size], label='Actual Values', marker='o')
plt.plot(y_pred_knn[:sample_size], label='Predicted Values', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Popularity')
plt.title('K-Nearest Neighbors Regressor: Actual vs. Predicted Values (Sample)')
plt.legend()
plt.show()

# XGBoost Regressor
xgb_model = XGBRegressor(random_state=666)
xgb_model.fit(X_train_scaled, y_train)
y_pred_xgb = xgb_model.predict(X_test_scaled)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'XGBoost Regressor MSE: {mse_xgb:.2f}')

# Actual vs. predicted values
sample_size = 150
plt.figure(figsize=(16, 8))
plt.plot(y_test.values[:sample_size], label='Actual Values', marker='o')
plt.plot(y_pred_xgb[:sample_size], label='Predicted Values', marker='x')
plt.xlabel('Sample Index')
plt.ylabel('Popularity')
plt.title('XGBoost Regressor: Actual vs. Predicted Values (Sample)')
plt.legend()
plt.show()
